{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soumya-Sri-Mutyala/Machine-Learning/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRIb1LvNaYyz"
      },
      "outputs": [],
      "source": [
        "Name: Soumya Sri Mutyala\n",
        "Net ID - xo3697\n",
        "Group ID - 10\n",
        "Group members - Gangareddy Nachu, Soumya Sri Mutyala"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDscPppwaYy1"
      },
      "source": [
        "## For Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlXydgEpaYy2"
      },
      "source": [
        "### https://www.kaggle.com/uciml/pima-indians-diabetes-database - Download the indian diabetes dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU6lWeE1aYy2"
      },
      "source": [
        "1.Import relevant commands for numpy, pandas, sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veyTEpqiaYy2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, read_csv, to_numeric\n",
        "from sklearn import cluster\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNy5gWbsaYy3"
      },
      "source": [
        "2.Using the appropriate pandas function, read the diabetes.csv into a dataframe. Pay good attention to the necessary arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqFJjZUtaYy3"
      },
      "outputs": [],
      "source": [
        "# declare predictors\n",
        "predictors = ['Pregnancies','SkinThickness','Insulin','Age','BMI','Glucose','BloodPressure','DiabetesPedigreeFunction']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5kRAIqfMaYy3",
        "outputId": "30875260-b727-4bfc-a528-a28193d6863f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Results\n",
            "          Logistic Regression  Naive Bayes       3NN\n",
            "Fold  1              0.218208     0.236994  0.148844\n",
            "Fold  2              0.221418     0.234443  0.146165\n",
            "Fold  3              0.212735     0.232996  0.141823\n",
            "Fold  4              0.227207     0.247467  0.157742\n",
            "Fold  5              0.219971     0.244573  0.136035\n",
            "Fold  6              0.217077     0.235890  0.151954\n",
            "Fold  7              0.219971     0.240232  0.162084\n",
            "Fold  8              0.217077     0.230101  0.138929\n",
            "Fold  9              0.221418     0.248915  0.143271\n",
            "Fold  10             0.212428     0.225434  0.140173\n",
            " \n",
            "Validation Results\n",
            "          Logistic Regression  Naive Bayes       3NN\n",
            "Fold  1              0.210526     0.210526  0.289474\n",
            "Fold  2              0.233766     0.272727  0.428571\n",
            "Fold  3              0.298701     0.272727  0.246753\n",
            "Fold  4              0.129870     0.181818  0.207792\n",
            "Fold  5              0.207792     0.233766  0.298701\n",
            "Fold  6              0.220779     0.272727  0.324675\n",
            "Fold  7              0.220779     0.246753  0.259740\n",
            "Fold  8              0.259740     0.233766  0.324675\n",
            "Fold  9              0.168831     0.168831  0.311688\n",
            "Fold  10             0.289474     0.315789  0.407895\n",
            "\n",
            "The Mean and Standard deviation Results of Training Set are given below:: \n",
            "\n",
            "The mean Error rate of Logistic Regression  0.21875099336640372\n",
            "The mean Error rate of Naive Bayes   0.23770442434939731\n",
            "The mean Error rate of 3NN  0.14670202353964684\n",
            "The standard deviation of Logistic Regression  0.004132595923210123\n",
            "The standard deviation of Naive Bayes  0.00720790530089228\n",
            "The standard deviation of 3NN  0.008020952917133783\n",
            "\n",
            "The Mean and Standard deviation Results of Validation Set are given below:: \n",
            "\n",
            "The mean Error rate of Logistic Regression  0.22402597402597402\n",
            "The mean Error rate of Naive Bayes   0.2409432672590567\n",
            "The mean Error rate of 3NN  0.30999658236500344\n",
            "The standard deviation of Logistic Regression  0.048588926731070375\n",
            "The standard deviation of Naive Bayes  0.042884170171581644\n",
            "The standard deviation of 3NN  0.06449632557689733\n"
          ]
        }
      ],
      "source": [
        "# Loop through all training and validations excel files and find the error rate using three algorithms\n",
        "x=10\n",
        "train_error_list=[]\n",
        "columns = ['Logistic Regression','Naive Bayes','3NN']\n",
        "train_error_log=[]\n",
        "train_error_gnb=[]\n",
        "train_error_knn=[]\n",
        "\n",
        "validation_error_list=[]\n",
        "validation_error_log=[]\n",
        "validation_error_gnb=[]\n",
        "validation_error_knn=[]\n",
        "\n",
        "\n",
        "    \n",
        "lists=[]\n",
        "for i in range(x):\n",
        "    i= i+1\n",
        "    if i<10:\n",
        "            train_data = read_csv('F:\\MachineLearning\\diabetes_10fold_train_val\\XT0'+str(i)+'.csv', sep=\",\")\n",
        "            validation_data = read_csv('F:\\MachineLearning\\diabetes_10fold_train_val\\XV0'+str(i)+'.csv',sep=\",\")\n",
        "    else:\n",
        "            train_data = read_csv('F:\\MachineLearning\\diabetes_10fold_train_val\\XT'+str(i)+'.csv', sep=\",\")\n",
        "            validation_data = read_csv('F:\\MachineLearning\\diabetes_10fold_train_val\\XV'+str(i)+'.csv',sep=\",\")\n",
        "        \n",
        "            \n",
        "    train_data.columns\n",
        "    train_data.head()\n",
        "     \n",
        "    train_data.drop('Outcome',axis=1)\n",
        "    X=train_data[predictors]\n",
        "    y = train_data['Outcome']\n",
        "    validation_data.drop('Outcome',axis=1)\n",
        "    X1 = validation_data[predictors]\n",
        "    y1 = validation_data['Outcome']\n",
        "#Logisctic Regresion approach    \n",
        "    logisticreg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# fit the model with data\n",
        "    logisticreg.fit(X, y)\n",
        "   \n",
        "\n",
        "    logerror = 1-logisticreg.score(X,y)\n",
        "    logerror1 = 1-logisticreg.score(X1,y1)\n",
        "    train_error_log.append(logerror)\n",
        "    validation_error_log.append(logerror1)\n",
        "    #print('The Error using Logistic Regression on Training data fold',i,'  ',error)\n",
        "    #print('The Error using Logistic Regression on Validation data fold',i,'  ',error1)\n",
        "    \n",
        "    \n",
        "    # instantiate the model (using the default parameters)\n",
        "    gnb = GaussianNB()\n",
        "    \n",
        "\n",
        "# fit the model with data\n",
        "    gnb.fit(X, y)\n",
        "    \n",
        "\n",
        "    gnberror = 1-gnb.score(X,y)\n",
        "    gnberror1 = 1-gnb.score(X1,y1)\n",
        "    train_error_gnb.append(gnberror)\n",
        "    validation_error_gnb.append(gnberror1)\n",
        "    #print('The Error using GaussianNB on Training data fold',i,'  ',error)\n",
        "    #print('The Error using GaussianNB on Validation data fold',i,'  ',error1)\n",
        "    \n",
        "   \n",
        "    \n",
        "    # instantiate the model (using the default parameters)\n",
        "    knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "\n",
        "# fit the model with data\n",
        "    knn.fit(X, y)\n",
        "    #knn.fit(X1, y1)\n",
        "\n",
        "    knnerror = 1-knn.score(X,y)\n",
        "    knnerror1 = 1-knn.score(X1,y1)\n",
        "    train_error_knn.append(knnerror)\n",
        "    validation_error_knn.append(knnerror1)\n",
        "    \n",
        "    #print('The Error using KNeighborsClassifier on Training data fold ',i,'  ',error)\n",
        "    #print('The Error using KNeighborsClassifier on Validation data fold',i,'  ',error1)\n",
        "    \n",
        "   \n",
        "   \n",
        "   \n",
        "    \n",
        "index=[]\n",
        "\n",
        "for i in range(len(train_error_log)):\n",
        "    train_error_list.append([train_error_log[i],train_error_gnb[i],train_error_knn[i]])\n",
        "    validation_error_list.append([validation_error_log[i],validation_error_gnb[i],validation_error_knn[i]])\n",
        "    index.append(\"Fold  \"+str(i+1))\n",
        "print(\"Training Results\")   \n",
        "df_train_error = pd.DataFrame(train_error_list,columns=columns,index=index)\n",
        "print(df_train_error.head(20)) \n",
        "print(\" \")\n",
        "print(\"Validation Results\")\n",
        " \n",
        "df_validation_error = pd.DataFrame(validation_error_list,columns=columns,index=index)\n",
        "print(df_validation_error.head(20)) \n",
        "print()\n",
        "\n",
        "print(\"The Mean and Standard deviation Results of Training Set are given below:: \")\n",
        "print()\n",
        "print('The mean Error rate of Logistic Regression ', np.mean(train_error_log))\n",
        "print('The mean Error rate of Naive Bayes  ', np.mean(train_error_gnb))\n",
        "print('The mean Error rate of 3NN ', np.mean(train_error_knn))\n",
        "print('The standard deviation of Logistic Regression ',np.std(train_error_log))\n",
        "print('The standard deviation of Naive Bayes ',np.std(train_error_gnb))\n",
        "print('The standard deviation of 3NN ',np.std(train_error_knn))\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"The Mean and Standard deviation Results of Validation Set are given below:: \")\n",
        "print()\n",
        "print('The mean Error rate of Logistic Regression ', np.mean(validation_error_log))\n",
        "print('The mean Error rate of Naive Bayes  ', np.mean(validation_error_gnb))\n",
        "print('The mean Error rate of 3NN ', np.mean(validation_error_knn))\n",
        "print('The standard deviation of Logistic Regression ',np.std(validation_error_log))\n",
        "print('The standard deviation of Naive Bayes ',np.std(validation_error_gnb))\n",
        "print('The standard deviation of 3NN ',np.std(validation_error_knn))\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-CdHB58aYy5"
      },
      "source": [
        "3.use naivebayes, logistic regression and 3-nn classifiers (library) to train on the training sets and \n",
        "compute training and validation errors for each fold (see the diabetes_10fold_train_val.zip file, XT01...XT10: training sets, XV01....XV10: corresponding validation sets). The target label is Outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6m8_6HlaYy5"
      },
      "source": [
        "4.is the error of naive bayes <0.2 with confidence 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_njRChpaYy5",
        "outputId": "e18cd6d6-1b2e-4c45-8336-59221a217128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.0191555268432793\n"
          ]
        }
      ],
      "source": [
        "#Standard deviation of naive bayes\n",
        "S= np.std(validation_error_gnb)\n",
        "#Mean of naive bayes\n",
        "m= np.mean(validation_error_gnb)\n",
        "k=10\n",
        "p0=0.2\n",
        "#calculate t test of naive bayes\n",
        "ttest = (math.sqrt(k)*(m-p0))/S\n",
        "print(ttest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyGxnXXpaYy6"
      },
      "source": [
        "5.have  naive bayes and knn the same error?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0kw6bc2aYy6",
        "outputId": "d6fe39cf-13a7-460d-d136-e716c738abb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5.3931713473041345\n"
          ]
        }
      ],
      "source": [
        "#Standard deviation of knn \n",
        "S= np.std(validation_error_knn)\n",
        "#Mean of knn \n",
        "m= np.mean(validation_error_knn)\n",
        "k=10\n",
        "p0=0.2\n",
        "#calculate t test of naive bayes\n",
        "ttest = (math.sqrt(k)*(m-p0))/(S)\n",
        "print(ttest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icgf2hOCaYy6"
      },
      "source": [
        "6.do the three classifiers have different errors?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ND-zs95aYy6",
        "outputId": "c85e74c1-31e5-4492-c40b-edc816e4a7d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.563664934330391\n"
          ]
        }
      ],
      "source": [
        "#Standard deviation of logistic Regression \n",
        "S= np.std(validation_error_log)\n",
        "#Mean of logistic Regression \n",
        "m= np.mean(validation_error_log)\n",
        "k=10\n",
        "p0=0.2\n",
        "#calculate t test of naive bayes\n",
        "ttest = (math.sqrt(k )*(m-p0))/(S)\n",
        "print(ttest)\n"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "5fcfcbf3ab639a932915d4e4e9ae6de753a7302c7552b1ae46492bbdcc42c405"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Assignment 1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}